==== To Remember ====

- Ensemble methods (Random Subspaces) might be super interesting to test out for the 1st dataset, training different classifiers for different
features (since our features are naturally separated) !!!!!

- For feature selection:
    * select classifiers for baseline
    * use feature selection keeping k best features along to the criteria
    * vary k and see when the performance starts lowering

- If we use PCA for dimensionality reduction we should talk in the report about the possibility of generalization error,
    which requires extra calculations which we decided not to no. (Remember elbow plot for PCA)
    == THEY DO WANT US TO APPLY PCA! == EVALUATING SOME MEASURES OVER SEVERAL NUMBER OF Ks (probably in clustering)

- DT prunning needed ?

pATTERN MINNINNG INTER GRUPOS ? (CHATO PA CRLH N OBRIGATORIO), FAZER COM TUDO É FDD, FEAT SELECT AJUDA A SELECIONAR VARIAVEIS CERTAS,
COM UNSUPERVISED NORMALMENTE N TERÍAMOS ISSO.

Comparar qualidade de unsupervised vs classificadores. (Classificadores fuck yeah)


=== To ASK ===
- Mooshak

- (!!!) Feat select, use that little features? (!!!)

- (!!!) Using only 10k for 2nd dataset is enough or not really?

- (!!!) Smote on the less frequent classes and undersample on the others? Is this cool? How much samples should we
generate at max ?

- (!!!) Um grafico por classifier é necessario ou podemos skippar nB ?

- Ensemble models - explicar abordagem, falta algo?

- DT (entender measures dont change with tree depth,, tried split=5 & split=2 (???) same stuff, => unimportent features)

